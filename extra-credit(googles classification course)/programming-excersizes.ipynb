{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Classification Google Course**\n",
    "\n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "**Thresholds & the confusion matrix**\n",
    "\n",
    "\n",
    "\n",
    "**Thresholds in Binary Classification**\n",
    "\n",
    "Ex: \n",
    "- you have a regression model for SPAM-EMAIL DETECTION\n",
    "- this model predicts a value between 0 and 1, which represents the probability that a given email is spam\n",
    "- Ex: prediction of 0.5 = 50% likelihood that a given email IS SPAM\n",
    "- Ex: prediction of 0.75 = 75% likelihood that the email IS SPAM\n",
    "\n",
    "to convert this model into a usable filter, you need to **convert the raw numerical output** (eg. 0.75) into one of the 2 categories: **\"spam\"** or **\"not spam\"**\n",
    "\n",
    "to do this you choose a threshold probability aka a **classification threshold**. so when the probability is above the threshold, the values are assigned to a positive class, the class we are testing for (spam). but when the probability is lower, it is assigned to the negative/alternative class (not spam). \n",
    "\n",
    "Ex: \n",
    "- an email = 0.99 (predicting that the email has a 99% chance of being spam)\n",
    "- another email = 0.51% (predicting it has a 51% chance of being spam) \n",
    "- If the threshold = 0.5, the model will classify **both emails as spam** \n",
    "- if the threshold = 0.95, **the email with 99% will be classified as spam**\n",
    "\n",
    "**Confusion Matrix**\n",
    "A tool that compares the models predicitons to the actual outcomes: \n",
    "\n",
    "                Actual Positive (Spam)                 Actual Negative (Not Spam)\n",
    "-----------------------------------------------------------------------------------------\n",
    "Predicted           TP: Correctly identified                    FP: Real email\n",
    "Positive                spam                                marked as spam\n",
    "\n",
    "\n",
    "Predicted               FN: Spam that                           TN: Correctly   \n",
    "Negative                got through                          identified real email\n",
    "\n",
    "- rows = predictions \n",
    "- columns = actual labels \n",
    "- the matrix helps analyze the types of errors and successes your model is making \n",
    "\n",
    "\n",
    "\n",
    "**Effect of Changing the Threshold**\n",
    "- raising the threshold generally: \n",
    "    - decreases both TP & FP\n",
    "    - increases FN\n",
    "- lowering the threshold does the opposite\n",
    "\n",
    "\n",
    "**types of datasets**\n",
    "- seperated: positives & negatives are easily distinguishable \n",
    "- unseparated: overlap between positives & negatives\n",
    "- imbalanced: very few examples of the positve class\n",
    "\n",
    "\n",
    "**CHECK UR UNDERSTANDING**\n",
    "1. Imagine a phishing or malware classification model where phishing and malware websites are in the class labeled 1 (true) and harmless websites are in the class labeled 0 (false). This model mistakenly classifies a legitimate website as malware. What is this called?\n",
    "    **FP**\n",
    "2. In general, what happens to the number of false positives when the classification threshold increases? What about true positives? Experiment with the slider above.\n",
    "    **Both TP & FP decrease**\n",
    "3. In general, what happens to the number of false negatives when the classification threshold increases? What about true negatives? Experiment with the slider above.\n",
    "    **Both true and false negatives increase.**\n",
    "\n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "**Accuracy, recall, precision, and related metrics**\n",
    "\n",
    "\n",
    "\n",
    "**Accuracy** \n",
    "- define: proportion of all predictions that were correct: \n",
    "\n",
    "            Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "- Good for: balanced datasets \n",
    "- Bad for: imbalanced datasets - can be misleading (ex: 99% accuracy from always predicting the majority class)\n",
    "- Use: as a general progress measure, but not alone for decision-making\n",
    "\n",
    "\n",
    "\n",
    "**Recall (True Positive Rate)** \n",
    "- define: proportion of actual positives correctly predicted:\n",
    "\n",
    "            Recall = (TP) / (TP + FN)\n",
    "- Good for: catching all positive cases (detecting disease, spam, invasive species)\n",
    "- Used when: FN are most costly than FP\n",
    "\n",
    "\n",
    "\n",
    "**False Positive Rate (FPR)** \n",
    "- define: proportion of actual negatives incorrectly classified as positives:\n",
    "\n",
    "            FPR = (FP) / (FP + TN)\n",
    "- Good for: measuring rate of false alarms\n",
    "- Used when: FP are more costly than FN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Precision** \n",
    "- define: proportion of predicted positives that are actually positive:\n",
    "\n",
    "            Percision = (TP) / (TP + FP)\n",
    "- Good for: making sure positive predictions are trustworthy (low false positives)\n",
    "- Used when:Accuracy of positive predictions is critcal \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Threshold Tradeoffs** \n",
    "- Raising the threshold -> increases precision, decreases recall. \n",
    "- Raising the threshold -> increases recall, decreases precision. \n",
    "- Precision vs Recall often have an inverse relationship \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Metric Selection Advice** \n",
    "\n",
    "Metric                    Use when...\n",
    "-----------------------------------------------------------------------------------------\n",
    "Accuracy                    Dataset is balanced; use w/ other metrics \n",
    "Recall                      FN are expensive (missing dangoerus cases)\n",
    "Precision                   FP are expensive (spam misclassifcation) \n",
    "FPR                         False alarms matter more than misses \n",
    "\n",
    "\n",
    "\n",
    "**CHECK UR UNDERSTANDING**\n",
    "1. RECALL.\n",
    "given TP = 5, FN = 2\n",
    "recall = (5)/(5+2) = 5/7 = 0.714\n",
    "2. PRECISION:\n",
    "given tp = 3, fp = 2\n",
    "precision = (3) / (3 + 2) = 3/5 = 0/6 \n",
    "3. Which metric to optimize (invasive species detection)?\n",
    "recall\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "bbefe49d5a115135"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
